{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "This notebook needs to be executed to create all the plots and tables of statistical tests that are referenced in the main paper.Rmd file. Output is created from the csv files located at ../results/single_run\n",
    "\n",
    "\n",
    "The initial results can be reproduced by running the shell script <run.zsh>\n",
    "\n",
    "All results of this notebook are saved in ../docs/rmd/plots and ../docs/rmd/tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'mean' from partially initialized module 'statistics' (most likely due to a circular import) (/Users/rmn/github/intelligent_information_systems_research_project/src/statistics.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple, List\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatistics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n",
      "File \u001b[0;32m~/github/intelligent_information_systems_research_project/src/statistics.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple, List\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatistics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'mean' from partially initialized module 'statistics' (most likely due to a circular import) (/Users/rmn/github/intelligent_information_systems_research_project/src/statistics.py)"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mannwhitneyu, normaltest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Tuple, List\n",
    "from statistics import mean\n",
    "from dataclasses import dataclass\n",
    "import csv\n",
    "import os\n",
    "\n",
    "%cd ~/github/intelligent_information_systems_research_project/seminar/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting setup\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "FIGSIZE_INCHES =  14, 9\n",
    "FIGSIZE_INCHES_LARGE =  16, 10\n",
    "TITLE_FONT_SIZE = 17\n",
    "TITLE_FONT_SIZE_LARGE = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set output paths\n",
    "\n",
    "PATH = \"../docs/rmd/\"\n",
    "TABLE_PATH = f\"{PATH}/tables\"\n",
    "PLOT_PATH =  f\"{PATH}/plots\"\n",
    "\n",
    "jobs = (\n",
    "    'os.makedirs(f\"{TABLE_PATH}/csv\")',\n",
    "    'os.makedirs(f\"{TABLE_PATH}/md\")',\n",
    "    'os.makedirs(f\"{PLOT_PATH}\")'\n",
    ")\n",
    "\n",
    "for job in jobs:\n",
    "    try:\n",
    "        exec(job)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "# set input paths\n",
    "\n",
    "DIR_TOURNAMENT = \"../results/single_run/e_lexicase\"\n",
    "DIR_ELEXICASE = \"../results/single_run/tournament\"\n",
    "\n",
    "\n",
    "tournament_files = os.listdir(DIR_TOURNAMENT)\n",
    "elexicase_files = os.listdir(DIR_ELEXICASE)\n",
    "\n",
    "if not len(tournament_files) == len(elexicase_files):\n",
    "    print(\"Warning - Unequal number of records!\\nVariable <TOTAL_RUNS> not set\")\n",
    "    TOTAL_RUNS = None\n",
    "    \n",
    "else:\n",
    "    TOTAL_RUNS = len(tournament_files)\n",
    "    print(f\"Total number of Runs: {TOTAL_RUNS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsv_to_df(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Return the results for ../results/single_run/<algorithm><id>.tsv as a pd.DataFrame\"\"\"\n",
    "\n",
    "    df = pd.read_csv(filepath_or_buffer=filepath, sep=\"\\t\", index_col=False, skipinitialspace=True)\n",
    "    \n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    rename_dict = {\n",
    "        \"avg\" : \"mean_training_error\",\n",
    "        \"std\" : \"std_training_error\",\n",
    "        \"min\" : \"min_training_error\",\n",
    "        \"max\" : \"max_training_error\",\n",
    "        \"elite_testing_mse\" : \"testing_error\",\n",
    "        \"elite_testing_err_std\" : \"std_testing_error\"\n",
    "        \n",
    "    }\n",
    "        \n",
    "    return df.rename(columns=rename_dict) \n",
    "        \n",
    "\n",
    "\n",
    "# read and store all log files into dataframes\n",
    "tournament_logs = []\n",
    "elexicase_logs = []\n",
    "\n",
    "for a, b in zip(tournament_files, elexicase_files):\n",
    "    tournament_logs.append(\n",
    "        tsv_to_df(f\"{DIR_TOURNAMENT}/{a}\")\n",
    "    )\n",
    "    elexicase_logs.append(\n",
    "        tsv_to_df(f\"{DIR_ELEXICASE}/{b}\")\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all individual logs\n",
    "\n",
    "for idx, (a, b) in enumerate(zip(tournament_logs, elexicase_logs)):\n",
    "    print(f\"{idx+1}.th Run:\\nTournament-Selection:\\n{a}\\nE-Lexicase-Selection:\\n{b}\\n--------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_master_record(dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Summarize and return the results from each individual dataframe into a master record\n",
    "    \"\"\"\n",
    "    \n",
    "    headers = dfs[0].columns.values.tolist()\n",
    "    ngens = len(dfs[0][\"gen\"])\n",
    "    \n",
    "    master = pd.DataFrame(0, index=np.arange(ngens), columns=headers)\n",
    "    \n",
    "    def mean_stddev(std_devs: List[float]) -> float:\n",
    "        \"\"\"returns the mean for a list of std_deviations \"\"\"\n",
    "        agg = 0.0\n",
    "        for std_dev in std_devs:\n",
    "            agg += std_dev ** 2\n",
    "        return sqrt(agg / len(std_devs))\n",
    "    \n",
    "    for header in headers:\n",
    "                \n",
    "        for gen in range(ngens):\n",
    "            \n",
    "            vals = []\n",
    "            \n",
    "            for df in dfs:\n",
    "                vals.append(\n",
    "                    float(df[header].iloc[gen])\n",
    "                )\n",
    "\n",
    "            if not \"std\" in header:\n",
    "                master.loc[gen,header] = mean(vals)\n",
    "            \n",
    "            else:\n",
    "                master.loc[gen,header] = mean_stddev(vals)\n",
    "                \n",
    "    return master\n",
    "\n",
    "\n",
    "\n",
    "master_tournmament = to_master_record(tournament_logs)\n",
    "master_elexicase = to_master_record(elexicase_logs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_tournmament.to_csv(path_or_buf=f\"{TABLE_PATH}/csv/master_tournament.csv\")\n",
    "master_tournmament.to_markdown(buf=f\"{TABLE_PATH}/md/master_tournament.md\")\n",
    "\n",
    "master_tournmament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_tournmament.describe().to_csv(path_or_buf=f\"{TABLE_PATH}/csv/master_tournament_descriptive.csv\")\n",
    "master_tournmament.describe().to_markdown(buf=f\"{TABLE_PATH}/md/master_tournament_descriptive.md\")\n",
    "\n",
    "master_tournmament.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_elexicase.to_csv(path_or_buf=f\"{TABLE_PATH}/csv/master_elexicase.csv\")\n",
    "master_elexicase.to_markdown(buf=f\"{TABLE_PATH}/md/master_elexicase.md\")\n",
    "\n",
    "master_elexicase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_elexicase.describe().to_csv(path_or_buf=f\"{TABLE_PATH}/csv/master_elexicase_descriptive.csv\")\n",
    "master_elexicase.describe().to_markdown(buf=f\"{TABLE_PATH}/md/master_elexicase_descriptive.md\")\n",
    "\n",
    "master_elexicase.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_cells(dfs: List[pd.DataFrame], header, row) -> List[float]:\n",
    "    \n",
    "    vals = []\n",
    "\n",
    "    for df in dfs:\n",
    "        \n",
    "        vals.append(\n",
    "            df[header].iloc[row]\n",
    "        )\n",
    "    \n",
    "    return vals\n",
    "\n",
    "LAST_ROW = len(tournament_logs[0]) - 1\n",
    "\n",
    "# aggregate training errors for elite models in last generation\n",
    "tournament_elite_training_errors = aggregate_cells(tournament_logs, \"min_training_error\", LAST_ROW)\n",
    "elexicase_elite_training_errors = aggregate_cells(elexicase_logs, \"min_training_error\", LAST_ROW)\n",
    "\n",
    "# aggregate elite model performance on testing data\n",
    "tournament_elite_testing_errors = aggregate_cells(tournament_logs, \"testing_error\", LAST_ROW)\n",
    "elexicase_elite_testing_errors = aggregate_cells(elexicase_logs, \"testing_error\", LAST_ROW)\n",
    "\n",
    "\n",
    "# aggregate size values\n",
    "\n",
    "tournament_elite_size = aggregate_cells(tournament_logs, \"elite_size\", LAST_ROW)\n",
    "elexicase_elite_size = aggregate_cells(elexicase_logs, \"elite_size\", LAST_ROW)\n",
    "\n",
    "# aggregate elite model performance on testing data\n",
    "tournament_avg_size = aggregate_cells(tournament_logs, \"avg_size\", LAST_ROW)\n",
    "elexicase_avg_size = aggregate_cells(elexicase_logs, \"avg_size\", LAST_ROW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if samples are normal distributed at alpha=5%, results are written to ../docs/rmd/tables/normal_dist_test.csv\"\n",
    "\n",
    "def is_normal_distr(vals: List[float], name: str, alpha:float=0.05) -> str:  \n",
    "    \"\"\"\n",
    "    Null Hypothesis: Sample comes from a normal distribution, \n",
    "    returns results as csv string:\n",
    "        <sample,statistic,p-value,alpha,normal_distributed>\n",
    "    \n",
    "    \"\"\"\n",
    "    statistic, pval = normaltest(vals)\n",
    "    return f\"{name},{statistic},{pval},{alpha},{pval >= alpha}\\n\"\n",
    "    \n",
    "\n",
    "csv_str = (\n",
    "    \"sample,statistic,p-value,alpha,normal_distributed\\n\" +\n",
    "    is_normal_distr(tournament_elite_training_errors, \"Tournament - Training Errors\") +\n",
    "    is_normal_distr(elexicase_elite_training_errors, \"E-Lexicase - Training Errors\") +\n",
    "    is_normal_distr(tournament_elite_testing_errors, \"Tournament - Testing Errors\") +\n",
    "    is_normal_distr(elexicase_elite_testing_errors, \"E-Lexicase - Testing Errors\")\n",
    ")\n",
    "\n",
    "print(csv_str)\n",
    "\n",
    "        \n",
    "with open(f\"../docs/rmd/tables/csv/normal_dist_test.csv\", \"w\") as fstr:\n",
    "    fstr.write(csv_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mannwhitneyu(sample_a: List[float], sample_b: List[float], alpha:float=0.05) -> Tuple[float,float]:\n",
    "    \"\"\"\n",
    "    performs a mann whitney u ranksum test for sample_a and sample_b, \n",
    "    returns the results as csv string\n",
    "        <test statistic and p-value>\n",
    "    \"\"\"\n",
    "    statistic, pval = mannwhitneyu(x = sample_a,y = sample_b)\n",
    "    print(f\"Statistic: {statistic}\\nPVal: {pval}\\nPVal < ALPHA: {pval < alpha}\")\n",
    "\n",
    "    if pval > alpha:\n",
    "        print(f\"Results supports H0 for alpha={alpha}\\n H0: The distribution underlying sample_a is the same as the distribution underlying sample_b\")\n",
    "\n",
    "    else:\n",
    "        print(f\"H0 can be rejected for alpha={alpha}\\nThe distribution underlying sample_a is NOT the same as the distribution underlying sample_b\")\n",
    "    \n",
    "    return statistic, pval\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Sample:\n",
    "    vals: List[float]\n",
    "    name: str\n",
    "\n",
    "        \n",
    "def mark(pval:float) -> str:\n",
    "        \"\"\"\n",
    "        mark pvalues for statistical significance:\n",
    "            alpha:\n",
    "                0.1   : *\n",
    "                0.05  : **\n",
    "                0.025 : ***\n",
    "        \"\"\"\n",
    "        s = str(pval)\n",
    "        \n",
    "        if pval < 0.1:\n",
    "            s += '*'\n",
    "        if pval < 0.05:\n",
    "            s += '*'\n",
    "        if pval < 0.025:\n",
    "            s += '*'\n",
    "            \n",
    "        return s        \n",
    "\n",
    "\n",
    "def mwu_csv_matrix(\n",
    "    samples: List[Sample],\n",
    "    alpha:float=0.05\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    returns the results of mwu test as a csv matrix\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # x 0 1 2 3\n",
    "    # 0\n",
    "    # 1\n",
    "    # 2\n",
    "    # 3\n",
    "    \n",
    "    \n",
    "    names = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        names.append(sample.name)\n",
    "    \n",
    "    matrix = [[None for _ in range(len(samples))] for _ in range(len(samples))]\n",
    "    \n",
    "    for ix, xsample in enumerate(samples):\n",
    "        for iy, ysample in enumerate(samples):\n",
    "            _, p = mannwhitneyu(x=xsample.vals, y=ysample.vals)\n",
    "            matrix[ix][iy] = p\n",
    "            \n",
    "    csv_str = \"{0},{1},{2},{3}\\n\".format(*[name for name in names])\n",
    "    \n",
    "    for row,name in zip(matrix, names):\n",
    "        csv_str += \"{},{},{},{},{}\\n\".format(name, *row)\n",
    "        \n",
    "    return csv_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MWU - Error\n",
    "\n",
    "samples = [\n",
    "    Sample(tournament_elite_training_errors, \"tournament_training_errors\"),\n",
    "    Sample(tournament_elite_testing_errors, \"tournament_testing_errors\"),\n",
    "    Sample(elexicase_elite_training_errors, \"elexicase_training_errors\"),\n",
    "    Sample(elexicase_elite_testing_errors, \"elexicase_testing_errors\") \n",
    "]\n",
    "\n",
    "\n",
    "csv = mwu_csv_matrix(samples)\n",
    "\n",
    "with open(\"../docs/rmd/tables/csv/mwu_matrix_error.csv\", \"w\") as f:\n",
    "    f.write(csv)\n",
    "\n",
    "\n",
    "\n",
    "pd.read_csv(open(\"../docs/rmd/tables/csv/mwu_matrix_error.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MWU - Size\n",
    "\n",
    "# MWU - Error\n",
    "\n",
    "samples_size = [\n",
    "    Sample(tournament_elite_size, \"tournament_elite_size\"),\n",
    "    Sample(elexicase_elite_size, \"elexicase_elite_size\"),\n",
    "    Sample(tournament_avg_size, \"tournament_avg_size\"),\n",
    "    Sample(elexicase_avg_size, \"elexicase_avg_size\") \n",
    "]\n",
    "\n",
    "\n",
    "csv = mwu_csv_matrix(samples_size)\n",
    "\n",
    "with open(\"../docs/rmd/tables/csv/mwu_matrix_size.csv\", \"w\") as f:\n",
    "    f.write(csv)\n",
    "\n",
    "\n",
    "\n",
    "pd.read_csv(open(\"../docs/rmd/tables/csv/mwu_matrix_size.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_boxplots(\n",
    "    sample_a: List[float],\n",
    "    sample_b: List[float],\n",
    "    title: str,\n",
    "    a_label:str, b_label: str, filename: str) -> None:\n",
    "\n",
    "    PATH = f\"../docs/rmd/plots/{filename}.png\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(*FIGSIZE_INCHES)\n",
    "    \n",
    "    plt.grid(visible=True, axis='both')\n",
    "\n",
    "    ax.boxplot(\n",
    "        x = [sample_a, sample_b],\n",
    "        labels=[a_label, b_label]\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, fontsize=TITLE_FONT_SIZE_LARGE)\n",
    "    ax.set_ylabel(\"MSE\")\n",
    "    plt.savefig(PATH)\n",
    "    plt.show()\n",
    "    \n",
    "def save_as_boxplots_all(\n",
    "    sample_a: List[float],\n",
    "    sample_b: List[float],\n",
    "    sample_c: List[float],\n",
    "    sample_d: List[float],\n",
    "    title: str,\n",
    "    a_label:str, b_label: str, c_label:str,d_label: str,\n",
    "    filename: str\n",
    ") -> None:\n",
    "\n",
    "    PATH = f\"../docs/rmd/plots/{filename}.png\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(*FIGSIZE_INCHES_LARGE)\n",
    "    \n",
    "    plt.grid(visible=True, axis='both')\n",
    "\n",
    "    ax.boxplot(\n",
    "        x = [sample_a, sample_b, sample_c, sample_d],\n",
    "        labels=[a_label, b_label, c_label, d_label]\n",
    "    )\n",
    "    \n",
    "    ax.set_title(title, fontsize=TITLE_FONT_SIZE_LARGE)\n",
    "    ax.set_ylabel(\"MSE\")\n",
    "    plt.savefig(PATH)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mannwhitneyu(tournament_elite_training_errors, elexicase_elite_training_errors)\n",
    "\n",
    "save_as_boxplots(\n",
    "    tournament_elite_training_errors,\n",
    "    elexicase_elite_training_errors,\n",
    "    \"Training Errors Distribution\",\n",
    "    \"Tournament-Selection\",\n",
    "    \"Epsilon-Lexicase-Selection\",\n",
    "    \"mean_training_errors_boxplot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mannwhitneyu(tournament_elite_testing_errors, elexicase_elite_testing_errors)\n",
    "\n",
    "save_as_boxplots(\n",
    "    tournament_elite_testing_errors,\n",
    "    elexicase_elite_testing_errors,\n",
    "    \"Testing Errors Distribution\",\n",
    "    \"Tournament-Selection\",\n",
    "    \"Epsilon-Lexicase-Selection\",\n",
    "    \"mean_testing_errors_boxplot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_boxplots_all(\n",
    "    sample_a=tournament_elite_testing_errors,\n",
    "    sample_b=tournament_elite_training_errors,\n",
    "    sample_c=elexicase_elite_testing_errors,\n",
    "    sample_d=elexicase_elite_training_errors,\n",
    "    title=\"Testing Errors Distribution\",\n",
    "    a_label=\"Tournament_Testing\",\n",
    "    b_label=\"Tournament_Training\",\n",
    "    c_label=\"E_Lexicase_Testing\",\n",
    "    d_label=\"E_Lexicase_Training\",\n",
    "    filename=\"mean_error_boxplot_all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing error gap\n",
    "\n",
    "def evolutionary_plot(\n",
    "    master_record: pd.DataFrame, \n",
    "    header_1: str, \n",
    "    header_2: str, \n",
    "    algorithm_name: str, \n",
    "    filename: str, \n",
    "    suptitle: str,\n",
    "    y_scale: Tuple[int, int]=(0,100)\n",
    "):\n",
    "    \n",
    "    \n",
    "    PATH = f\"../docs/rmd/plots/{filename}.png\"\n",
    "    \n",
    "    X = np.arange(\n",
    "        min(master_record[\"gen\"]),\n",
    "        max(master_record[\"gen\"] +1)\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(*FIGSIZE_INCHES)\n",
    "    \n",
    "    \n",
    "    ax.plot(X, master_record[header_1], label=header_1)\n",
    "    ax.plot(X, master_record[header_2], label=header_2)\n",
    "    \n",
    "    ax.set_title(f\"{suptitle} - {algorithm_name}\", fontsize=TITLE_FONT_SIZE_LARGE)\n",
    "    ax.set_ylim(*y_scale)\n",
    "    \n",
    "    ax.set_xlabel(\"generations\")\n",
    "    ax.set_ylabel(\"MSE\")\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.grid(visible=True, axis='both')\n",
    "    plt.savefig(PATH)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# testing error gap\n",
    "\n",
    "def evolutionary_masterplot(\n",
    "    master_record_1: pd.DataFrame,\n",
    "    master_record_2: pd.DataFrame,\n",
    "    total_runs: int,\n",
    "    header_1: str,\n",
    "    header_2: str,\n",
    "    algorithm_1_name: str,\n",
    "    algorithm_2_name: str,\n",
    "    filename: str,\n",
    "    suptitle: str,\n",
    "    y_label: str,\n",
    "    y_scale: Tuple[int, int]=(0,100),\n",
    "):\n",
    "    \n",
    "    \n",
    "    PATH = f\"../docs/rmd/plots/{filename}.png\"\n",
    "    \n",
    "    X = np.arange(\n",
    "        min(master_record_1[\"gen\"]),\n",
    "        max(master_record_2[\"gen\"] +1)\n",
    "    )\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "    fig.set_size_inches(*FIGSIZE_INCHES_LARGE)\n",
    "    \n",
    "    \n",
    "    ax1.plot(X, master_record_1[header_1], label=header_1)\n",
    "    ax1.plot(X, master_record_1[header_2], label=header_2)\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(X, master_record_2[header_1], label=header_1)\n",
    "    ax2.plot(X, master_record_2[header_2], label=header_2)\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    ax1.set_title(algorithm_1_name, fontsize=TITLE_FONT_SIZE)\n",
    "    ax2.set_title(algorithm_2_name, fontsize=TITLE_FONT_SIZE)\n",
    "    \n",
    "    ax1.set_ylim(*y_scale)\n",
    "    ax2.set_ylim(*y_scale)\n",
    "    \n",
    "    ax1.set_xlabel(\"generations\")\n",
    "    ax1.set_ylabel(y_label)\n",
    "    \n",
    "    ax2.set_xlabel(\"generations\")\n",
    "    ax2.set_ylabel(\"MSE\")\n",
    "    \n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.suptitle(f\"{suptitle} for {total_runs} total Runs\", fontsize=TITLE_FONT_SIZE_LARGE)\n",
    "    \n",
    "    plt.savefig(PATH)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# testing error gap\n",
    "\n",
    "def evolutionary_combined_masterplot(\n",
    "    master_record_1: pd.DataFrame,\n",
    "    master_record_2: pd.DataFrame,\n",
    "    total_runs: int,\n",
    "    header_1: str,\n",
    "    header_2: str,\n",
    "    algorithm_1_name: str,\n",
    "    algorithm_2_name: str,\n",
    "    filename: str,\n",
    "    suptitle: str,\n",
    "    y_label: str,\n",
    "    y_scale: Tuple[int, int]=(0,100),\n",
    "):\n",
    "    \n",
    "    \n",
    "    PATH = f\"../docs/rmd/plots/{filename}.png\"\n",
    "    \n",
    "    X = np.arange(\n",
    "        min(master_record_1[\"gen\"]),\n",
    "        max(master_record_2[\"gen\"] +1)\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "    fig.set_size_inches(*FIGSIZE_INCHES_LARGE)\n",
    "    \n",
    "    \n",
    "    ax.plot(X, master_record_1[header_1], \"b\" , label=f\"{algorithm_1_name}_{header_1}\")\n",
    "    ax.plot(X, master_record_1[header_2], \"g\" , label=f\"{algorithm_1_name}_{header_2}\")\n",
    "    ax.plot(X, master_record_2[header_1], \"y\", label=f\"{algorithm_2_name}_{header_1}\")\n",
    "    ax.plot(X, master_record_2[header_2], \"r\",label=f\"{algorithm_2_name}_{header_2}\")\n",
    "    \n",
    "    ax.grid(True)\n",
    "\n",
    "    \n",
    "    ax.set_ylim(*y_scale)\n",
    "    ax.set_ylim(*y_scale)\n",
    "    \n",
    "    ax.set_ymargin(1.5)\n",
    "    \n",
    "    ax.set_xlabel(\"generations\")\n",
    "    ax.set_ylabel(y_label)\n",
    "\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.suptitle(f\"{suptitle} for {total_runs} total Runs\", fontsize=TITLE_FONT_SIZE)\n",
    "    \n",
    "    plt.savefig(PATH)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Error - Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolutionary_plot(\n",
    "    master_record=master_tournmament,\n",
    "    header_1=\"min_training_error\",\n",
    "    header_2=\"testing_error\",\n",
    "    algorithm_name=\"Tournament Selection\",\n",
    "    filename=\"tournament_evolution\",\n",
    "    suptitle=\"Mean Error\"\n",
    ")\n",
    "evolutionary_plot(\n",
    "    master_record=master_elexicase,\n",
    "    header_1=\"min_training_error\",\n",
    "    header_2=\"testing_error\",\n",
    "    algorithm_name=\"Epsilon-Lexicase Selection\",\n",
    "    filename=\"elexicase_evolution\",\n",
    "    suptitle=\"Mean Error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolutionary_masterplot(\n",
    "    master_record_1=master_tournmament,\n",
    "    master_record_2=master_elexicase,\n",
    "    total_runs=TOTAL_RUNS,\n",
    "    header_1=\"min_training_error\",\n",
    "    header_2=\"testing_error\",\n",
    "    algorithm_1_name=\"Tournament Selection\",\n",
    "    algorithm_2_name=\"Epsilon-Lexicase Selection\",\n",
    "    filename=\"mean_error_subplotted\",\n",
    "    suptitle=\"Mean Error\",\n",
    "    y_label=\"MSE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "evolutionary_combined_masterplot(\n",
    "    master_record_1=master_tournmament,\n",
    "    master_record_2=master_elexicase,\n",
    "    total_runs=TOTAL_RUNS,\n",
    "    header_1=\"min_training_error\",\n",
    "    header_2=\"testing_error\",\n",
    "    algorithm_1_name=\"Tournament Selection\",\n",
    "    algorithm_2_name=\"Epsilon-Lexicase Selection\",\n",
    "    filename=\"mean_error_combined\",\n",
    "    suptitle=\"Mean Error\",\n",
    "    y_label=\"MSE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Size Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolutionary_masterplot(\n",
    "    master_record_1=master_tournmament,\n",
    "    master_record_2=master_elexicase,\n",
    "    total_runs=TOTAL_RUNS,\n",
    "    header_1=\"avg_size\",\n",
    "    header_2=\"elite_size\",\n",
    "    algorithm_1_name=\"Tournament Selection\",\n",
    "    algorithm_2_name=\"Epsilon-Lexicase Selection\",\n",
    "    filename=\"size_subplotted\",\n",
    "    suptitle=\"Mean Size\",\n",
    "    y_label=\"size\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolutionary_combined_masterplot(\n",
    "    master_record_1=master_tournmament,\n",
    "    master_record_2=master_elexicase,\n",
    "    total_runs=TOTAL_RUNS,\n",
    "    header_1=\"avg_size\",\n",
    "    header_2=\"elite_size\",\n",
    "    algorithm_1_name=\"Tournament Selection\",\n",
    "    algorithm_2_name=\"Epsilon-Lexicase Selection\",\n",
    "    filename=\"size_combined\",\n",
    "    suptitle=\"Mean Size\",\n",
    "    y_label=\"size\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add, commit and push to github remote repo\n",
    "# ! git add * && git commit -m \"working in jupyter notebook\" && git push"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "76ccdc9e1609e02ff543acc18e37045188f863069557f4e8891716419ee222bc"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
