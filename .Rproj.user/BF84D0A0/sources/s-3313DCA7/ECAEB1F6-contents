% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12pt,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\usepackage{float}
\usepackage[numbers]{natbib}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}
\usepackage{float}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Analyzing the influence of Selection on Genetic Programming's
Generalization ability in Symbolic Regression:

A comparison of \(\epsilon\)-Lexicase Selection and Tournament Selection

--------------------------------------------------------

Student: Roman Höhn

Student ID: 2712497

Supervisor: David Wittenberg

--------------------------------------------------------

03.996.3299 Seminar Information Systems

Chair of Business Administration and Computer Science

Johannes Gutenberg University Mainz

Summerterm 2022}
\author{}
\date{\vspace{-2.5em}Date of Submission: 2022-06-20}

\begin{document}
\maketitle

\newpage{}

{
\setcounter{tocdepth}{3}
\tableofcontents
}
\newpage

\hypertarget{abstract}{%
\section{Abstract}\label{abstract}}

\ldots{}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Genetic programming (GP), a subfield of evolutionary computation (EC),
is a metaheuristic that is used to search for computer programs that
solve a given problem by simulating the process of darwinian evolution.
The basic principle of GP is to gradually evolve solutions by repeatedly
selecting parent solutions from a randomized population of computer
programs based on a fitness metric. Then, genetic operators are applied
on the selected solutions to genereate new offspring candidate
solutions.By repeating this process over many generations, GP acts as a
guided search for high fitness solutions throughout the decision space.

A unique feature of GP among other evolutionary optimization procedures
is the possibility to evolve solutions of variable length.

The overall performance of GP can depend strongly on the choice of its
underlying operators, one crucial component in this is the operator for
parent selection.

Tournament Selection is a commonly used selection operator in EC and is
the most used operator in GP systems (Fang and Li, 2010, p. 181). A
parent solution is selected by randomly sampling \(k\) individuals from
the current population into a tournament pool and then the solution with
the highest fitness score from the tournament pool is selected (Fang and
Li, 2010, p. 182).

Lexicase selection has been suggested as an alternative to tournament
selection that is not based on aggregating fitness scores. It samples
\(n\) test cases in random order and then eliminates solutions from the
selection pool on a per test case basis if they are not performing on an
elite level (Helmuth, Spector and Matheson, 2015, p. 1).

Since regular Lexicase selection has been shown to perform suboptimal on
continous-valued optimization problems, a modified variation called
\(\epsilon\)-Lexicase selection has been suggested for symbolic
regression by La Cava et. al (2016). Here, \(\epsilon\)-Lexicase
selection has shown itself to outperform both in overall performance
while showing only negligible computational overhead (La Cava, Spector
and Danai, 2016, p. 747).

The goal of symbolic regression is to find a mathematical model for an
observed set of datapoints (Paris, Robilliard and Fonlupt, 2004, p.
794). Symbolic Regression has been one of the first GP applications and
to this day is an actively studied and highly relevant area of research
(Poli, Langdon and McPhee, 2008, p. 114). In most symbolic regression
problems, little to no a priori knowledge about the optimal form and
structure of the target function is available. The ability of GP to
optimize for model structure as well as for parameters has lead to it
being one of the most prevalent methods used in the domain of symbolic
regressionv(Paris, Robilliard and Fonlupt, 2004, p. 795).

An important quality of all supervised machine learning applications,
including GP, is the ability to not only optimize performance for the
test cases a model is trained on but to also perform well on previously
unseen cases, this is refered to as generalization. In most real world
applications of symbolic regression only a small subset of labeled data
is available for training. The aim is to produce a model that not only
accurately predicts the provided training data but can also predict
previously unseen cases with high precision (Gonçalves, 2016, p. 6). A
model that is extensivley optimized on the provided training data, may
overfit to this data sample which may lead to a decrease in
generalization.

This research project tries to answer the question if the usage of
\(\epsilon\)-Lexicase selection influences the generalization behaviour
of programs that are evolved using GP for symbolic regression if
compared to programs that are evolved using traditional tournament
selection.

\hypertarget{current-state-of-research}{%
\section{Current State of Research}\label{current-state-of-research}}

Lexicase selection has been developed specifically for the purpose of
solving problems with GP that require solutions to perform optimal on a
wide range of different test cases, so called uncompromising problems
(Helmuth, Spector and Matheson, 2015, p. 1).

Traditional GP selection methods, e.g.~tournament selection, most
commonly compute the fitness of a program as the mean of its error for
each individual fitness case. One downside to this approach is that the
total amount of information that is available for the evolutionary
search is reduced from a wide range of individual errors to a single
metric. \citeauthor{6920034} LALALA

More recent research suggests \(\epsilon\)-Lexicase selection as an
alternative selection operator that can improves overall performance of
GP system for continous-valued problems in comparison to the
traditionally used selection methods tournament selection and standard
lexicase selection (La Cava, Spector and Danai, 2016, p. 741). In
comparison to other selection operators, populations that are evolved
using variations of the lexicase selection operator show a very high
degree of genetic diversity which might be a key contributer to the
improved performance (Helmuth, Spector and Matheson, 2015, p. 1) (La
Cava, Spector and Danai, 2016, p. 745).

The performance increase of \(\epsilon\)-Lexicase selection for symbolic
regression problems has been demonstrated and reported for many
benchmark problems which led to widespread adaption of it in symbolic
regression applications (La Cava, Spector and Danai, 2016, pp.
744--745).

\#TODO: Generalization as an open topic of interest (O'Neill \emph{et
al.}, 2010)

\hypertarget{theoretical-foundations}{%
\section{Theoretical Foundations}\label{theoretical-foundations}}

In GP, candidate solution are computer programs that consist of
terminals and functions which are commonly represented as nodes and
leaves inside a tree structure.

Epsilon-Lexicase Selection\ldots{}

Formula\ldots{}

\hypertarget{experimental-study}{%
\section{Experimental study}\label{experimental-study}}

\hypertarget{methodology}{%
\subsection{Methodology}\label{methodology}}

To study the influence of selection on generalization I selected a
dataset about energy efficiency in buildings that is part of the UC
Irvine Machine Learning Repository (Dua and Graff, 2017). The dataset
contains eight individual building attributes that map to two different
outcomes, heating and cooling load, for N=768 cases (Tsanas and Xifara,
2012).

Using two otherwise identical GP systems, one deploying tournament
selection and the other \(\epsilon\)-lexicase selection, the objective
is to find a computer program that best predicts the outcome variable
heating load \((Y1)\)\footnote{The symbolic regression is performed on
  one of the two provided outcome variables, the variable Cooling Load
  will be excluded.} of the buildings using a subset of the eight
building attributes \((X1,..,X8)\) for input.

The specific meaning of all attributes inside the dataset are described
in table 1.

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-1}Overview - Energy Heating Dataset}
\centering
\begin{tabular}[t]{l|l}
\hline
\textbf{Variable} & \textbf{Description}\\
\hline
X1 & Relative Compactness\\
\hline
X2 & Surface Area\\
\hline
X3 & Wall Area\\
\hline
X4 & Roof Area\\
\hline
X5 & Overall Height\\
\hline
X6 & Orientation\\
\hline
X7 & Glazing Area\\
\hline
X8 & Glazing Area Distribution\\
\hline
y1 & Heating Load\\
\hline
y2 & Cooling Load\\
\hline
\end{tabular}
\end{table}

To measure the generalization ability of each model the dataset will be
randomly split in half, resulting in a training and testing dataset each
containing 384 individual cases.\\

Each model will be evolved by traditional GP using only the fitness
cases present in the training dataset. For each generation the highest
fitness model of the current population will be tested with the
previously unseen fitness cases that are part of the testing dataset.
For each run of the experiment statistics will be collected on fitness
which will form the basis of my further statistical analysis.

Since GP is a stochastic optimization algorithm, the basic experiment
will be run for a total of 50 times to ensure a fair and meaningful
comparison based on a large number of runs for both algorithms.

For each run of the experiment the dataset will be randomly split in
half as described, both models are then trained and tested using the
exact same set of fitness cases.

The statistical analysis of the collected data will first focus on
examining the hypothesis that, on average, the usage of
\(\epsilon\)-Lexicase selection will result in models that perform
significantly different than models that are evolved using tournament
selection if both are evaluated on previously unseen fitnesscases:

\begin{quote}
\(H0_{testing}\): The distribution underlying the samples of testing
errors produced by torunament selection is the same as the distribution
underlying samples of testing errors produced by \(\epsilon\)-lexicase
selection
\end{quote}

Since data on the fitness of each model during the training phase is
also collected, an additional analysis will be conducted on the
difference in performance between both selection methods during the
training phase of the experiment:

\begin{quote}
\(H0_{training}\): The distribution underlying the samples of training
errors produced by torunament selection is the same as the distribution
underlying samples of training errors produced by \(\epsilon\)-lexicase
selection
\end{quote}

Both hypothesis will be tested by performing a Mann-Whitney U Test using
a level of significance of \(\alpha=0.05\). \#TODO: Source! and in depth
explanation

To further examine the difference in generalization behaviour, the mean
testing and training errors over each generation will be visualized for
both algorithms. The specific aim of this visualization is to explore
the mechanism of overfitting and to examine if differences between both
methods can be detected.

All GP experiments will be implemented by using the python programming
language in conjunction with DEAP, a framework for distributed
evolutionary algorithms that implements various tools and algorithms for
genetic programming (Fortin \emph{et al.}, 2012).

\hypertarget{fitness-evaluation}{%
\subsection{Fitness Evaluation}\label{fitness-evaluation}}

The fitness \(f\) for each model will be based on the mean squared error
(MSE) over all fitness cases for prediction and the empirically measured
values as described by eq.1.

\begin{equation}
\tag{eq. 1}
MSE = \frac{1}{n} * \sum_{i=1}^{n} (Y_i - \hat{Y_1})^2
\end{equation}

where:

\begin{itemize}
\tightlist
\item
  \(n\): total number of test cases
\item
  \(Y_i\): empirical value for case \(i\)
\item
  \(\hat{Y_1}\): predicted value for case \(i\).
\end{itemize}

The resulting fitness function \(f\) for an individual program \(i\) is
descibed by eq 2:

\begin{equation}
\tag{eq. 2}
 f(i, \tau ) = \frac{1}{N} * \sum_{t \epsilon \tau} (y_t - y\hat{}_t(i, x_t))^2 
\end{equation}

where\footnote{Naming and notation was adopted from (La Cava, Spector
  and Danai, 2016)}:

\begin{itemize}
\tightlist
\item
  \(\tau\): the set of \(N\) fitness cases
\item
  \(y_t\): empirical value of the target for case \(t\)
\item
  \(y\hat{}_t(i, x_t)\): predicted value for the target for case \(t\)
  by running the program \(i\) with the total set of input variables
  \(x_t\)
\end{itemize}

\hypertarget{gp-settings}{%
\subsection{GP Settings}\label{gp-settings}}

The basic parameters for GP of both models are presented in table 2.

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-2}Evolutionary Parameters}
\centering
\begin{tabular}[t]{l|l}
\hline
\textbf{Parameter} & \textbf{Value}\\
\hline
Population Size & 500\\
\hline
Number of Generations & 100\\
\hline
Mutation Rate & 20\%\\
\hline
Crossover Rate & 80\%\\
\hline
Tournament Size & 3\\
\hline
Elite Size & 0\\
\hline
\end{tabular}
\end{table}

The primitve set consists of the terminals that are listed in table 3
and the functions that are listed in table 4. To avoid runtime errors I
implemented protected version of the operators for division, natural
logarithm and square root (Koza, 1992, pp. 82--83).

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-3}Terminals}
\centering
\begin{tabular}[t]{l|l}
\hline
\textbf{Terminal} & \textbf{Description}\\
\hline
X1 & Relative Compactness\\
\hline
X2 & Surface Area\\
\hline
X3 & Wall Area\\
\hline
X4 & Roof Area\\
\hline
X5 & Overall Height\\
\hline
X6 & Orientation\\
\hline
X7 & Glazing Area\\
\hline
X8 & Glazing Area Distribution\\
\hline
random\_int & Ephemeral Constant (integer)\\
\hline
random\_float & Ephemeral Constant(float)\\
\hline
\end{tabular}
\end{table}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-4}Functions}
\centering
\begin{tabular}[t]{l|r}
\hline
\textbf{Function} & \textbf{Arity}\\
\hline
Addition & 2\\
\hline
Subtraction & 2\\
\hline
Multiplication & 2\\
\hline
Negation & 1\\
\hline
Sine & 1\\
\hline
Cosine & 1\\
\hline
Protected Division & 2\\
\hline
Protected Natural Logarithm & 1\\
\hline
Protected Square Root & 1\\
\hline
\end{tabular}
\end{table}

GP operators are represented in table 5. Both genetic operators use a
static limit to control for the height of the reulting trees (Koza,
1992, p. 104). Individual programs are initialized by using the ramped
half-and-half method, 50\% of the population are created by using the
Growth algorithm and the remaining 50\% are created by using the Full
algorithm (Koza, 1992, p. 93).

The crossover operator implemented by DEAP randomly selects a crossover
point in each individual and exchanges each subtree with the point as
root between each individual (Fortin \emph{et al.}, 2012). Mutation also
randomly selects a point in the tree individual, it then replaces the
subtree below that point as a root by the expression generated using the
full grow initialization method (Fortin \emph{et al.}, 2012).

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-5}GP Operators}
\centering
\begin{tabular}[t]{l|l|r}
\hline
\textbf{Operator} & \textbf{Implementation} & \textbf{Static.Height.Limit}\\
\hline
Initilization & Ramped Half/Half & 2\\
\hline
Crossover & One Point Crossover & 17\\
\hline
Mutation & Uniform Mutation & 17\\
\hline
\end{tabular}
\end{table}

\#TODO: Explain Koza's major steps (define terminals, functions,
fitness, etc.)\ldots{} (Koza, 1992, p. 240 - ?)

\#TODO: Ephemeral constant (Koza, 1992, p. 243)

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{descriptive-statistics}{%
\subsection{Descriptive Statistics}\label{descriptive-statistics}}

Tables 6 and 7 summarize the results for all fitness scores collected
over 50 total runs of the experiment\footnote{All floating point numbers
  in text have been rounded to 3 decimal places}.

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-6}Summary - Tournament Selection}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lrrrrrrrr}
\toprule
\textbf{X} & \textbf{gen} & \textbf{nevals} & \textbf{mean\_training\_error} & \textbf{std\_training\_error} & \textbf{min\_training\_error} & \textbf{max\_training\_error} & \textbf{testing\_error} & \textbf{std\_testing\_error}\\
\midrule
count & 101.0 & 101.000 & 1.010000e+02 & 1.010000e+02 & 101.000 & 1.010000e+02 & 100.000 & 100.000\\
mean & 50.0 & 420.666 & 2.831341e+27 & 4.434935e+29 & 15.726 & 1.401758e+30 & 15.600 & 27.535\\
std & 29.3 & 8.112 & 2.033187e+28 & 3.209685e+30 & 11.752 & 1.016010e+31 & 10.115 & 13.633\\
min & 0.0 & 415.360 & 5.046633e+07 & 3.773790e+09 & 6.437 & 1.860963e+10 & 6.911 & 14.069\\
25\% & 25.0 & 418.920 & 1.003161e+12 & 1.511367e+14 & 7.976 & 5.015776e+14 & 8.549 & 18.311\\
\addlinespace
50\% & 50.0 & 419.860 & 1.457550e+17 & 2.097844e+19 & 10.918 & 5.529960e+19 & 11.274 & 24.081\\
75\% & 75.0 & 421.060 & 1.729104e+20 & 2.726844e+22 & 18.521 & 8.631720e+22 & 18.275 & 31.196\\
max & 100.0 & 500.000 & 2.028240e+29 & 3.203717e+31 & 75.002 & 1.014120e+32 & 56.752 & 84.613\\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-7}Summary - Epsilon-Lexicase Selection}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lrrrrrrrr}
\toprule
\textbf{X} & \textbf{gen} & \textbf{nevals} & \textbf{mean\_training\_error} & \textbf{std\_training\_error} & \textbf{min\_training\_error} & \textbf{max\_training\_error} & \textbf{testing\_error} & \textbf{std\_testing\_error}\\
\midrule
count & 101.0 & 101.000 & 1.010000e+02 & 1.010000e+02 & 101.000 & 1.010000e+02 & 100.000 & 100.000\\
mean & 50.0 & 420.856 & 1.280691e+15 & 2.022923e+17 & 22.266 & 6.403455e+17 & 22.199 & 35.087\\
std & 29.3 & 8.091 & 1.284480e+16 & 2.028913e+18 & 17.085 & 6.422410e+18 & 15.761 & 18.765\\
min & 0.0 & 416.380 & 2.111084e+05 & 1.450062e+07 & 9.679 & 7.428547e+07 & 10.419 & 20.038\\
25\% & 25.0 & 419.160 & 1.632374e+07 & 1.894222e+09 & 11.400 & 8.001532e+09 & 12.236 & 23.582\\
\addlinespace
50\% & 50.0 & 420.080 & 2.276037e+08 & 3.225811e+10 & 14.105 & 1.125624e+11 & 14.774 & 27.229\\
75\% & 75.0 & 421.080 & 9.303091e+09 & 1.259322e+12 & 24.406 & 4.471997e+12 & 24.272 & 38.829\\
max & 100.0 & 500.000 & 1.290910e+17 & 2.039070e+19 & 82.257 & 6.454560e+19 & 80.244 & 114.622\\
\bottomrule
\end{tabular}}
\end{table}

\hypertarget{todo-rewrite-based-on-new-results}{%
\section{TODO: Rewrite based on new
results!}\label{todo-rewrite-based-on-new-results}}

\hypertarget{inferential-statistics}{%
\subsection{Inferential Statistics}\label{inferential-statistics}}

Figure 1 visualizes the distribution of minimum fitness scores achieved
by both GP systems after finishing 100 full generations for all 50 runs
of the experiment. Similiar to Figure 1, Figure 2 shows the distribution
of fitness scores that were collected for the best performing models of
each run if computed on the unseen fitness cases of the testing dataset.

\begin{figure}
\centering
\includegraphics{./plots/mean_training_errors_boxplots.png}
\caption{Mean Training Errors}
\end{figure}

\begin{figure}
\centering
\includegraphics{./plots/mean_testing_errors_boxplots.png}
\caption{Mean Testing Errors}
\end{figure}

\hypertarget{todo-interpretation}{%
\section{TODO: Interpretation}\label{todo-interpretation}}

The four samples from figure 1 and 2 have been tested for normal
distribution by computing the D'Agostino and Pearson test (D'Agostino,
1971) (D'Agostino and Pearson, 1973) to determine if they satisfy the
requirements of the students t-test. Not all samples test positive for a
normal distribution therefore a Mann-Whitney U ranksum test will be
performed to test for statistical significance. Interestingly the two
samples that are produced by \(\epsilon\)-Lexicase selection pass the
the test for normal distribution while both samples for tournament
selection do not. The results are summarized in table 8.

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-8}Normal Distribution Tests}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lrrrl}
\toprule
\textbf{sample} & \textbf{statistic} & \textbf{p.value} & \textbf{alpha} & \textbf{normal\_distributed}\\
\midrule
Tournament - Training Errors & 33.564 & 0.000 & 0.05 & False\\
E-Lexicase - Training Errors & 1.167 & 0.558 & 0.05 & True\\
Tournament - Testing Errors & 28.490 & 0.000 & 0.05 & False\\
E-Lexicase - Testing Errors & 0.746 & 0.689 & 0.05 & True\\
\bottomrule
\end{tabular}}
\end{table}

The results of the Mann-Whitney-U test are summarized in table 9.

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-9}Results of MWU - Ranksum Test}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lrrrr}
\toprule
\textbf{ } & \textbf{tournament\_training\_errors} & \textbf{tournament\_testing\_errors} & \textbf{elexicase\_training\_errors} & \textbf{elexicase\_testing\_errors}\\
\midrule
tournament\_training\_errors & 1.000 & 0.551 & 0.000 & 0.000\\
tournament\_testing\_errors & 0.551 & 1.000 & 0.000 & 0.000\\
elexicase\_training\_errors & 0.000 & 0.000 & 1.000 & 0.275\\
elexicase\_testing\_errors & 0.000 & 0.000 & 0.275 & 1.000\\
\bottomrule
\end{tabular}}
\end{table}

To further examine the generalization behaviour based on both selection
operators I plotted the mean fitness scores for both algorithms in
figure 3 and 4.

\begin{figure}
\centering
\includegraphics{./plots/mean_error_subplotted.png}
\caption{Mean Testing Errors}
\end{figure}

\begin{figure}
\centering
\includegraphics{./plots/mean_error_combined.png}
\caption{Mean Testing Errors}
\end{figure}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

\hypertarget{limitations}{%
\section{Limitations}\label{limitations}}

\newpage

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\leavevmode\vadjust pre{\hypertarget{ref-DAgostino1971AnOT}{}}%
D'Agostino, R.B. (1971) {``An omnibus test of normality for moderate and
large size samples,''} \emph{Biometrika}, 58, pp. 341--348.

\leavevmode\vadjust pre{\hypertarget{ref-DAgostino1973TestsFD}{}}%
D'Agostino, R.B. and Pearson, E.S. (1973) {``Tests for departure from
normality,''} in.

\leavevmode\vadjust pre{\hypertarget{ref-Dua:2019}{}}%
Dua, D. and Graff, C. (2017) {``{UCI} machine learning repository.''}
University of California, Irvine, School of Information; Computer
Sciences. Available at: \url{http://archive.ics.uci.edu/ml}.

\leavevmode\vadjust pre{\hypertarget{ref-10.1007ux2f978-3-642-16493-4_19}{}}%
Fang, Y. and Li, J. (2010) {``A review of tournament selection in
genetic programming,''} in Cai, Z. et al. (eds.) \emph{Advances in
computation and intelligence}. Berlin, Heidelberg: Springer Berlin
Heidelberg, pp. 181--192.

\leavevmode\vadjust pre{\hypertarget{ref-DEAP_JMLR2012}{}}%
Fortin, F.-A. \emph{et al.} (2012) {``{DEAP}: Evolutionary algorithms
made easy,''} \emph{Journal of Machine Learning Research}, 13, pp.
2171--2175.

\leavevmode\vadjust pre{\hypertarget{ref-Gonalves2016AnEO}{}}%
Gonçalves, I. (2016) {``An exploration of generalization and overfitting
in genetic programming: Standard and geometric semantic approaches,''}
in.

\leavevmode\vadjust pre{\hypertarget{ref-6920034}{}}%
Helmuth, T., Spector, L. and Matheson, J. (2015) {``Solving
uncompromising problems with lexicase selection,''} \emph{IEEE
Transactions on Evolutionary Computation}, 19(5), pp. 630--643.
doi:\href{https://doi.org/10.1109/TEVC.2014.2362729}{10.1109/TEVC.2014.2362729}.

\leavevmode\vadjust pre{\hypertarget{ref-koza_main}{}}%
Koza, J.R. (1992) \emph{Genetic programming: On the programming of
computers by means of natural selection}. Cambridge, MA, USA: MIT Press.
Available at: \url{http://mitpress.mit.edu/books/genetic-programming}.

\leavevmode\vadjust pre{\hypertarget{ref-epsilon_lexicase_main}{}}%
La Cava, W., Spector, L. and Danai, K. (2016) {``Epsilon-lexicase
selection for regression,''} in \emph{Proceedings of the genetic and
evolutionary computation conference 2016}. New York, NY, USA:
Association for Computing Machinery (GECCO '16), pp. 741--748.
doi:\href{https://doi.org/10.1145/2908812.2908898}{10.1145/2908812.2908898}.

\leavevmode\vadjust pre{\hypertarget{ref-open_issues_gp}{}}%
O'Neill, M. \emph{et al.} (2010) {``Open issues in genetic
programming,''} \emph{Genetic Programming and Evolvable Machines}, 11,
pp. 339--363.
doi:\href{https://doi.org/10.1007/s10710-010-9113-2}{10.1007/s10710-010-9113-2}.

\leavevmode\vadjust pre{\hypertarget{ref-10.1007ux2f978-3-540-24621-3_22}{}}%
Paris, G., Robilliard, D. and Fonlupt, C. (2004) {``Exploring
overfitting in genetic programming,''} in Liardet, P. et al. (eds.)
\emph{Artificial evolution}. Berlin, Heidelberg: Springer Berlin
Heidelberg, pp. 267--277.

\leavevmode\vadjust pre{\hypertarget{ref-poli08:fieldguide}{}}%
Poli, R., Langdon, W.B. and McPhee, N.F. (2008) \emph{A field guide to
genetic programming}. Published via \texttt{http://lulu.com}; freely
available at \texttt{http://www.gp-field-guide.org.uk}. Available at:
\url{https://digitalcommons.morris.umn.edu/cgi/viewcontent.cgi?article=1001\&context=cs_facpubs}.

\leavevmode\vadjust pre{\hypertarget{ref-fe8fa39e88a040bbacba5a465c48043f}{}}%
Tsanas, A. and Xifara, A. (2012) {``Accurate quantitative estimation of
energy performance of residential buildings using statistical machine
learning tools,''} \emph{Energy and buildings}, 49, pp. 560--567.
doi:\href{https://doi.org/10.1016/j.enbuild.2012.03.003}{10.1016/j.enbuild.2012.03.003}.

\end{CSLReferences}

\end{document}
