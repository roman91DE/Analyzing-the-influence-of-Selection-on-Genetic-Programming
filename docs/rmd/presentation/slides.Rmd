---
title: "Analyzing the influence of Selection on Genetic Programming's Generalization ability in Symbolic Regression"
subtitle: "A comparison of epsilon-lexicase Selection and Tournament Selection"
author: "Roman Hoehn, B.Sc. Wirtschaftspaedagogik"
date: "2022-06-29"
output:
  beamer_presentation:
    fonttheme: "structurebold"
    keep_tex: true
    fig_caption: yes
    number_sections: true
    slide_level: 2
toc: yes
bibliography: ../bib/bib/ref.bib
csl: ../bib/csl/harvard1.csl
header-includes:
   - \usepackage{float}
---


```{r setup, include=FALSE}
library(magrittr)
knitr::opts_chunk$set(echo = TRUE)
```



# Introduction

## Research Question

* Does the usage of $\epsilon$-lexicase parent selection influence the generalization behaviour of genetic programming in symbolic regression if compared to tournament selection?

## Genetic Programming 

* A metaheuristic that searches for computer programs that solve a given problem^[@koza_main]
* Evolutionary Algorithm: Simulates the process of Darwinian evolution
* Unique Features:
  * Evolve solutions of variable length and structure
  * Solutions are typically represented by recursive tree structures


## Parent Selection

### Torunament Selection

* Most commonly used selection operator in Genetic Programming (GP)^[@10.1007/978-3-642-16493-4_19, p.181]
* Intuition: High chance for "generalist" solutions to be selected since it is based on aggregated fitness scores

### $\epsilon$-Lexicase Selection

* Objective: Create Selection method for uncompromising, continous-valued symbolic regression problems ^[@6920034, p.12]
* Performance increases have been demonstrated in many benchmarking problems ^[@epsilon_lexicase_main, p.744-745]
* Intuition: Higher chance for "specialist" solutions to be selected since it is decided on a per case basis


## Related Concepts

### Symbolic Regression

* Task: Find a mathematical model that fits a given set of datapoints
* One of the first applications of GP described by @koza_main
* High relevance: GP can outperform state-of-the-art machine learning algorithms like gradient boosting ^[@Orzechowski_2018]

### Generalization

* Main objective in most supervised machine learning problems: Achieve good performance for unseen data
* Challenge: Avoid overfitting to training data
* Little attention has been paid to generalization in GP ^[@open_issues_gp, @generalisation_in_gp]



# Experimental Study


## Benchmark problem

UC Irvine Machine Learning Repository: Prediction of energy efficiency in buildings ^[@Dua:2019]

```{r vars_dataset, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
fig.pos="H"

knitr::kable(
  read.csv("../tables/csv/variables_energy_heating.csv"),
  digits=3,
  caption="Overview - Energy Heating data set"
) 
```


## Experiment

### Single run

* Total dataset ($N=768$) is randomly split into a training and testing dataset (50:50)
* Fitness metric: Mean squared Error (MSE)
* Train two models using GP with the training dataset only, one using tournament selection and the other $\epsilon$-lexicase selection
* For each generation: Select elite model and compute its fitness for the testing dataset

### Full experiment

* Stochastic algorithm: Repeat the basic experiment for 50 total runs
* Collect and aggregate results for training error, testing error and program length


## Hypothesis Testing

**Does the usage of $\epsilon$-lexicase parent selection influence the generalization behavior of genetic programming in symbolic regression if compared to tournament selection?**

1. Test for differences in average fitness between both algorithms
2. Test for differences in average fitness between training and testing data


# Results


## Finding 1

* The differences in average fitness of the final solutions between tournament selection and $\epsilon$-lexicase selection are highly statistical significant ($\alpha=0.01$)
* Tournament selection-based GP achieves a higher fitness for both training and testing data
* Unexpected results based on the reviewed literature [@epsilon_lexicase_main], [@https://doi.org/10.48550/arxiv.1709.05394]


## Distribution of Fitness

![](../plots/mean_error_boxplot_all.png)


## Example

Model evolved by tournament selection after 100 generations:

![](../plots/example_model.png)


## Finding 2

* The gap between training and testing error is not statistically significant for both selection algorithms
* Both algorithms achieve a slightly better performance for the training data
* Good generalization: No evidence of overfitting


## Evolution of Fitness

![](../plots/mean_error_combined.png)


## Statistical Test

```{r pvals_fitness, echo=FALSE, warning=FALSE}
df <- read.csv("../tables/csv/mwu_matrix_error.csv")

knitr::kable(
  df,
  format="latex",
  digits=3,
  caption="Mean Error - P-Values (MWU)"
) %>% kableExtra::kable_styling(font_size = 4) 

```

## Program Growth

* So far: No proof of differences in generalization
* New approach: Program growth as a possible indicator for overfitting?
* Theory: Minimum description length principle (MDLP) ^[@wang_wagner_rondinelli_2019, p. 268]
* Downside: Growth/Bloat is no clear indicator of overfitting ^[@bloat_overfitting_gp, p. 8] 


## Evolution of Size

\begin{figure}
\centering
\includegraphics{../plots/size_subplotted.png}
\end{figure}


## Finding 3

* GP typical growth behaviour for both operators
* Solutions grow at a similiar rate in each generation
* No statistically significant differences in overall program size based on selection


# Conclusions

* Experiment did not yield evidence for differences in generalization behavior between tournament and $\epsilon$-lexicase selection
* The performance of tournament selection is significantly higher than that of $\epsilon$-lexicase selection for the selected symbolic regression problem
* No evidence for differences in growth behavior between both algorithms



# Limitations and open Questions

1. Configuration of evolutionary parameters
2. Results are based on a single symbolic regression
2. Limited by computational resources


# Appendix


## Evolutionary Parameters

```{r ea_config, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
fig.pos="H"

knitr::kable(
  read.csv("../tables/csv/gp_parameters.csv"),
  format="latex",
  digits=3,
  caption="Evolutionary Parameters"
)
```

## Primitive Set {.allowframebreaks} 

```{r primitive_table, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
fig.pos="b"

knitr::kable(
  read.csv("../tables/csv/primitives.csv"),
  format="latex",
  digits=3,
  caption="Function Set"
) 
```
\framebreak 

```{r function_table, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
fig.pos="t"

knitr::kable(
  read.csv("../tables/csv/terminals.csv"),
  format="latex",
  digits=3,
  caption="Terminal Set"
) 
```




# References {.allowframebreaks} 







