@article{Saini:2021ta,
  abstract     = {In genetic programming, parent selection methods are employed to select promising candidate individuals from the current generation that can be used as parents for the next generation. These algorithms can affect, sometimes indirectly, whether or not individuals containing certain programming constructs, such as loops, are selected and propagated in the population. This in turn can affect the chances that the population will produce a solution to the problem. In this paper, we present the results of the experiments using three different parent selection methods on four benchmark program synthesis problems. We analyze the relationships between the selection methods, the numbers of individuals in the population that make use of loops, and success rates. The results show that the support for the selection of specialists is associated both with the use of loops in evolving populations and with higher success rates.},
  author       = {Saini, Anil Kumar and Spector, Lee},
  url          = {https://doi.org/10.1007/s10710-021-09417-5},
  date         = {2021},
  doi          = {10.1007/s10710-021-09417-5},
  isbn         = {1573-7632},
  journaltitle = {Genetic Programming and Evolvable Machines},
  number       = {4},
  pages        = {495--509},
  title        = {Relationships between parent selection methods, looping constructs, and success rate in genetic programming},
  volume       = {22},
}

@book{poli08:fieldguide,
  abstract  = {3 Getting Ready to Run Genetic Programming 3.1 Step 1: Terminal Set 3.2 Step 2: Function Set 3.2.1 Closure 3.2.2 Sufficiency 3.2.3 Evolving Structures other than Programs 3.3 Step 3: Fitness Function 3.4 Step 4: GP Parameters 3.5 Step 5: Termination and solution designation 4 Example Genetic Programming Run 29 4.1 Preparatory Steps 4.2 Step-by-Step Sample Run 4.2.1 Initialisation 4.2.2 Fitness Evaluation 4.2.3 Selection, Crossover and Mutation 4.2.4 Termination and Solution Designation Part II Advanced Genetic Programming 5 Alternative Initialisations and Operators in Tree-based GP 5.1 Constructing the Initial Population 5.1.1 Uniform Initialisation 5.1.2 Initialisation may Affect Bloat 5.1.3 Seeding 5.2 GP Mutation 5.2.1 Is Mutation Necessary? 5.2.2 Mutation Cookbook 5.3 GP Crossover 5.4 Other Techniques 6 Modular, Grammatical and Developmental Tree-based GP 6.1 Evolving Modular and Hierarchical Structures 6.1.1 Automatically Defined Functions 6.1.2 Program Architecture and Architecture-Altering 6.2 Constraining Structures 6.2.1 Enforcing Particular Structures 6.2.2 Strongly Typed GP 6.2.3 Grammar-based Constraints 6.2.4 Constraints and Bias 6.3 Developmental Genetic Programming 6.4 Strongly Typed Autoconstructive GP with PushGP 7 Linear and Graph Genetic Programming 7.1 Linear Genetic Programming 7.1.1 Motivations 7.1.2 Linear GP Representations 7.1.3 Linear GP Operators 7.2 Graph-Based Genetic Programming 7.2.1 Parallel Distributed GP (PDGP) 7.2.2 PADO 7.2.3 Cartesian GP 7.2.4 Evolving Parallel Programs using Indirect Encodings 8 Probabilistic Genetic Programming 8.1 Estimation of Distribution Algorithms 8.2 Pure EDA GP 8.3 Mixing Grammars and Probabilities 9 Multi-objective Genetic Programming 9.1 Combining Multiple Objectives into a Scalar Fitness Function 9.2 Keeping the Objectives Separate 9.2.1 Multi-objective Bloat and Complexity Control 9.2.2 Other Objectives 9.2.3 Non-Pareto Criteria 9.3 Multiple Objectives via Dynamic and Staged Fitness Functions 9.4 Multi-objective Optimisation via Operator Bias 10 Fast and Distributed Genetic Programming 10.1 Reducing Fitness Evaluations/Increasing their Effectiveness 10.2 Reducing Cost of Fitness with Caches 10.3 Parallel and Distributed GP are Not Equivalent 10.4 Running GP on Parallel Hardware 10.4.1 Master slave GP 10.4.2 GP Running on GPUs 10.4.3 GP on FPGAs 10.4.4 Sub-machine-code GP 10.5 Geographically Distributed GP 11 GP Theory and its Applications 11.1 Mathematical Models 11.2 Search Spaces 11.3 Bloat 11.3.1 Bloat in Theory 11.3.2 Bloat Control in Practice Part III Practical Genetic Programming 12 Applications 12.1 Where GP has Done Well 12.2 Curve Fitting, Data Modelling and Symbolic Regression 12.3 Human Competitive Results the Humies 12.4 Image and Signal Processing 12.5 Financial Trading, Time Series, and Economic Modelling 12.6 Industrial Process Control 12.7 Medicine, Biology and Bioinformatics 12.8 GP to Create Searchers and Solvers Hyper-heuristics 12.9 Entertainment and Computer Games 12.10 The Arts 12.11 Compression 13 Troubleshooting GP 13.1 Is there a Bug in the Code? 13.2 Can you Trust your Results? 13.3 There are No Silver Bullets 13.4 Small Changes can have Big Effects 13.5 Big Changes can have No Effect 13.6 Study your Populations 13.7 Encourage Diversity 13.8 Embrace Approximation 13.9 Control Bloat 13.10 Checkpoint Results 13.11 Report Well 13.12 Convince your Customers 14 Conclusions Part IV Tricks of the Trade A Resources A.1 Key Books A.2 Key Journals A.3 Key International Meetings A.4 GP Implementations A.5 On-Line Resources B TinyGP B.1 Overview of TinyGP B.2 Input Data Files for TinyGP B.3 Source Code B.4 Compiling and Running TinyGP Bibliography Index},
  author    = {Poli, Riccardo and Langdon, William B. and McPhee, Nicholas Freitag},
  publisher = {Published via \texttt{http://lulu.com} and freely available at \texttt{http://www.gp-field-guide.org.uk}},
  url       = {https://digitalcommons.morris.umn.edu/cgi/viewcontent.cgi?article=1001&context=cs_facpubs},
  date      = {2008},
  keywords  = {genetic algorithms,genetic programming,cartesian genetic programming,automatic programming,machine learning,artificial intelligence,evolutionary computation,GPU},
  note      = {(With contributions by J. R. Koza)},
  title     = {A field guide to genetic programming},
}

@inproceedings{10.1145/2908812.2908898,
  abstract  = {Lexicase selection is a parent selection method that considers test cases separately, rather than in aggregate, when performing parent selection. It performs well in discrete error spaces but not on the continuous-valued problems that compose most system identification tasks. In this paper, we develop a new form of lexicase selection for symbolic regression, named ε-lexicase selection, that redefines the pass condition for individuals on each test case in a more effective way. We run a series of experiments on real-world and synthetic problems with several treatments of ε and quantify how ε affects parent selection and model performance. ε-lexicase selection is shown to be effective for regression, producing better fit models compared to other techniques such as tournament selection and age-fitness Pareto optimization. We demonstrate that ε can be adapted automatically for individual test cases based on the population performance distribution. Our experiments show that ε-lexicase selection with automatic ε produces the most accurate models across tested problems with negligible computational overhead. We show that behavioral diversity is exceptionally high in lexicase selection treatments, and that ε-lexicase selection makes use of more fitness cases when selecting parents than lexicase selection, which helps explain the performance improvement.},
  author    = {La Cava, William and Spector, Lee and Danai, Kourosh},
  location  = {Denver, Colorado, USA},
  publisher = {Association for Computing Machinery},
  url       = {https://doi.org/10.1145/2908812.2908898},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},
  date      = {2016},
  doi       = {10.1145/2908812.2908898},
  isbn      = {9781450342063},
  keywords  = {parent selection,regression,system identification,genetic programming},
  pages     = {741--748},
  series    = {GECCO '16},
  title     = {Epsilon-Lexicase Selection for Regression},
}

@inproceedings{10.1145/3321707.3321828,
  abstract  = {The lexicase parent selection method selects parents by considering performance on individual data points in random order instead of using a fitness function based on an aggregated data accuracy. While the method has demonstrated promise in genetic programming and more recently in genetic algorithms, its applications in other forms of evolutionary machine learning have not been explored. In this paper, we investigate the use of lexicase parent selection in Learning Classifier Systems (LCS) and study its effect on classification problems in a supervised setting. We further introduce a new variant of lexicase selection, called batch-lexicase selection, which allows for the tuning of selection pressure. We compare the two lexicase selection methods with tournament and fitness proportionate selection methods on binary classification problems. We show that batch-lexicase selection results in the creation of more generic rules which is favorable for generalization on future data. We further show that batch-lexicase selection results in better generalization in situations of partial or missing data.},
  author    = {Aenugu, Sneha and Spector, Lee},
  location  = {Prague, Czech Republic},
  publisher = {Association for Computing Machinery},
  url       = {https://doi.org/10.1145/3321707.3321828},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  date      = {2019},
  doi       = {10.1145/3321707.3321828},
  isbn      = {9781450361118},
  keywords  = {lexicase selection,parent selection,learning classifier systems},
  pages     = {356--364},
  series    = {GECCO '19},
  title     = {Lexicase Selection in Learning Classifier Systems},
}

@article{6920034,
  author       = {Helmuth, Thomas and Spector, Lee and Matheson, James},
  date         = {2015},
  doi          = {10.1109/TEVC.2014.2362729},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  number       = {5},
  pages        = {630--643},
  title        = {Solving Uncompromising Problems With Lexicase Selection},
  volume       = {19},
}

